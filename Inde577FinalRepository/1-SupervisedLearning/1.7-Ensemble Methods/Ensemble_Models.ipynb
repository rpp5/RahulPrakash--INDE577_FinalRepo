{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods in Machine Learning\n",
    "\n",
    "Ensemble methods are a foundational concept in machine learning, where the principle is straightforward yet powerful: combine the predictions from multiple models to create a final model that is more robust and accurate than any individual component model. This approach is akin to seeking advice from a group of experts rather than relying on a single person's opinion. Ensemble methods can reduce variance, bias, or improve predictions, and are broadly divided into several types:\n",
    "\n",
    "## Bootstrap Aggregating (Bagging)\n",
    "\n",
    "Bootstrap aggregating, often referred to as bagging, is a method that involves creating multiple versions of a predictor and using these to get an aggregated predictor. The data used for training each model is resampled with replacement from the entire training dataset. This means that each model sees a slightly different slice of the data, leading to variance among their predictions. However, when combined, these models tend to cancel out their individual variances, leading to a more stable and robust prediction.\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Random Forests take the concept of bagging a step further by introducing randomness into the tree building process. Here's how it works:\n",
    "\n",
    "1. Start with a bootstrap sample of the data.\n",
    "2. Fit a decision tree to this sample. When splitting a node during the construction of the tree, a random subset of the features is considered for the split.\n",
    "3. Each tree in the forest is built from a sample drawn with replacement (bootstrap sample) from the training set.\n",
    "4. After a large number of trees are built using this method, the predictions of the individual trees are averaged to make the final prediction.\n",
    "\n",
    "The randomization ensures that the high variance of the trees is averaged out, yielding a model with better generalization performance.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At each iteration of the training process, a new learner is added, while all existing learners are kept unchanged. All learners are weighted based on their performance (e.g., accuracy), and after a weak learner is added, the data weights are readjusted. Misclassified input data gains a higher weight, and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, starts by fitting an initial model (usually a decision tree) to the data. Then it increases the weights of the misclassified cases so that subsequent models focus more on difficult cases. This process is repeated sequentially, each time updating the weights of instances based on the last prediction. AdaBoost is sensitive to noisy data and outliers and can be less robust in such situations.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a sophisticated version of boosting. It builds the model in a stage-wise fashion like AdaBoost but generalizes the procedure by allowing optimization of an arbitrary differentiable loss function. In Gradient Boosting, trees are built sequentially, with each tree being trained to correct the mistakes of the previous one. Unlike AdaBoost, which changes the weights for misclassified data points at every iteration, Gradient Boosting performs this by fitting the new model to the residual errors made by the previous predictor.\n",
    "\n",
    "Let's fit these Ensemble Learning models and see how well they fit and can make predictions on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.22.4)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/rahulprakash/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"salary.csv\")\n",
    "\n",
    "# Convert all columns to float, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Replace NaN values with the mean of each column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Replace infinite values with the maximum non-infinite value from each column\n",
    "for column in df.columns:\n",
    "    if np.isinf(df[column]).any():\n",
    "        max_value = df[df[column] != np.inf][column].max()\n",
    "        df[column].replace([np.inf, -np.inf], max_value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 33360094460699.086\n",
      "Root Mean Squared Error: 5775819.8085379265\n"
     ]
    }
   ],
   "source": [
    "#Regression random forest\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded with data\n",
    "features = [\"Age\", \"GP\", \"MP\", \"FG\", \"FG%\", \"3P\", \"3P%\", \"eFG%\", \"TRB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\",\n",
    "            \"Total Minutes\", \"PER\", \"TS%\", \"3PAr\", \"FTr\", \"ORB%\", \"DRB%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\", \"TOV%\", \"USG%\",\n",
    "            \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\"]\n",
    "X = df[features].values\n",
    "y = df[\"Salary\"].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "forest_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "forest_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = forest_regressor.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error and Root Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our RMSE is quite low and suggests that under a random forest model, we can predict one's salary within an average of $5 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       0.78      0.71      0.74        49\n",
      "   Expensive       0.78      0.75      0.76        56\n",
      "      Medium       0.45      0.53      0.49        36\n",
      "\n",
      "    accuracy                           0.68       141\n",
      "   macro avg       0.67      0.66      0.67       141\n",
      "weighted avg       0.69      0.68      0.69       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification random forest\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already loaded with data\n",
    "features = [\"Age\", \"GP\", \"MP\", \"FG\", \"FG%\", \"3P\", \"3P%\", \"eFG%\", \"TRB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"PF\", \"PTS\",\n",
    "            \"Total Minutes\", \"PER\", \"TS%\", \"3PAr\", \"FTr\", \"ORB%\", \"DRB%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\", \"TOV%\", \"USG%\",\n",
    "            \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\"]\n",
    "X = df[features].values\n",
    "df['Salary_Category'] = pd.qcut(df['Salary'], q=3, labels=[\"Cheap\", \"Medium\", \"Expensive\"])\n",
    "y = df[\"Salary_Category\"].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Handle class imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=10)\n",
    "rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, random forest works well for classification as well and we correctly can classify a player's salary into high, medium, and low with an average accuracy of around 70%.\n",
    "\n",
    "Let's now fit a boost model and fit both AdaBoost and Gradient Boost models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Model Accuracy: 0.6453900709219859\n"
     ]
    }
   ],
   "source": [
    "#Ada boost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the AdaBoost classifier with a decision stump\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the AdaBoost model\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "print(f\"AdaBoost Model Accuracy: {ada_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Model Accuracy: 0.7163120567375887\n"
     ]
    }
   ],
   "source": [
    "#Gradient boost\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gradient_boost = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gradient_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Model Accuracy: {gb_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our Gradient Boost model fits our data and can make predictions more accurately than our AdaBoost model. As we've seen earlier, our data and the relationship that our predictors have with salary is quite complex, which is probably why our AdaBoost model is less accurate than our Gradient Boost model since AdaBoost is sensitive to noisy data and can be less robust in such situations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
